{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6116858",
   "metadata": {},
   "source": [
    "## Variable length sequence LSTM full script\n",
    "\n",
    "In this script, the whole process of loading data, training and testing the LSTM is written.\n",
    "The script is as generalized as it can get, since the variables of the model change based on the underlying dataset.\n",
    "This has been done to allow for the addition of new actions and data without having to change the code.\n",
    "Since it is a variable length sequence LSTM, ragged tensors are used.\n",
    "The predicting logic is a fixed length LSTM with 8 frame length. Being it real-time, it is not possible to predict variable length sequence, since one does not now when the sequence is completed. A challenge will be finding the sweet spot in terms of sequence length (for now it is 8 frames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ed5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing needed libraries\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the mediapipe model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "#defining a few functions to detect, draw and extract keypoints\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    #this function takes in the image and the model and returns the prediction results\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results, drawing_spec_circle, drawing_spec_line):\n",
    "    #this function takes in the image and results and draws mediapipe landmarks on the picture\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, landmark_drawing_spec=drawing_spec_circle, connection_drawing_spec=drawing_spec_line)\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, landmark_drawing_spec=drawing_spec_circle, connection_drawing_spec=drawing_spec_line)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, landmark_drawing_spec=drawing_spec_circle, connection_drawing_spec=drawing_spec_line)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, landmark_drawing_spec=drawing_spec_circle, connection_drawing_spec=drawing_spec_line)\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    #this function takes in the prediction results and returns the array with the extracted keypoints to be saved\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "def visualize_probabilities(res, actions, input_frame, colors):\n",
    "    #this function takes in the action probabilities and actions, and draws the coloured probability rectangles on the picture\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*200), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the action categories and paths\n",
    "\n",
    "actions = []\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"DATA\")\n",
    "\n",
    "for action in os.listdir(DATA_PATH):\n",
    "    actions.append(action)\n",
    "    \n",
    "actions = np.array(actions)\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "labels, temp_points = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a temporary solution to the initializing ragged tensors problem is to initialize with a standard value and then deleting it at\n",
    "#the end. I didn't find any better solution since documentation on ragged tensors is not so clean\n",
    "temporary_ragged = tf.ragged.constant([[[3, 1, 4, 1], [5, 9, 2], [6], []]], tf.double)\n",
    "\n",
    "#for every action, loops and loads data for training\n",
    "for action in actions: \n",
    "    for num_sequence in os.listdir(os.path.join(DATA_PATH, action)):\n",
    "        temp_points = []\n",
    "        for point in os.listdir(os.path.join(DATA_PATH, action, num_sequence)):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, num_sequence, point), allow_pickle = True)\n",
    "            temp_points.append(res)       \n",
    "            \n",
    "        temporary_ragged = tf.concat([temporary_ragged, tf.expand_dims(np.array(temp_points), axis = 0)], axis = 0)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "#skips the first tensor, that is the one we used to intialize the variable\n",
    "dataset = temporary_ragged[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6246a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing train and test sets (not really the cleanest way, ragged tensors don't seem too friendly to handle).\n",
    "#In the future I should add data augmentation as well, but it will probably be not clean to do that with ragged tensors\n",
    "\n",
    "y = to_categorical(labels).astype(int)\n",
    "train_ind = int(dataset.shape[0]*0.9)\n",
    "X_train = dataset[:train_ind]\n",
    "X_test = dataset[train_ind:]\n",
    "y_train = y[:train_ind]\n",
    "y_test = y[train_ind:]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da640329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the Keras model. Different models have been tested, for now with the dataset we have this has performed the best\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[None, 1662], ragged=True),\n",
    "    tf.keras.layers.LSTM(64, activation = 'tanh', dropout=0.2, return_sequences = True),\n",
    "    tf.keras.layers.LSTM(128, activation = 'tanh', dropout=0.2, return_sequences = True),\n",
    "    tf.keras.layers.LSTM(64, activation = 'tanh', dropout=0.2),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(len(actions), activation = 'softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining compiler and callback to stop training and 97% accuracy\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "\n",
    "ACCURACY_THRESHOLD = 0.97\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_epoch_end(self, epoch, logs={}): \n",
    "        if(logs.get('categorical_accuracy') > ACCURACY_THRESHOLD):   \n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))   \n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callb = myCallback()\n",
    "\n",
    "#training the model\n",
    "model.fit(X_train, y_train, epochs = 4000, callbacks = callb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0adbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using confusion matrix to determine how well the model performs on the test set (false positive and negatives)\n",
    "yhat = model.predict(X_test)\n",
    "ytrue = np.argmax(y_test, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis = 1).tolist()\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing out the accuracy\n",
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "model.save('model_weights\\\\5_actions_696(to_test).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d06b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the weights of a previously saved model\n",
    "model.load_weights('model_weights\\\\5_actions_741(to_test).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ee10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing in real-time\n",
    "\n",
    "colors = [(0,0,0), (255,255,255), (0,255,0), (255,0,0), (0,0,255), (125, 125, 125), (125, 0, 200)]\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = [0]\n",
    "threshold = 0.95\n",
    "\n",
    "#choosing the camera to use\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#drawing specs for face dots\n",
    "drawing_spec_circle = mp_drawing.DrawingSpec()\n",
    "drawing_spec_circle.circle_radius = 1\n",
    "drawing_spec_circle.thickness = 1\n",
    "drawing_spec_circle.color = (0,0,255)\n",
    "\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "#drawing specs for face connections\n",
    "drawing_spec_line = mp_drawing.DrawingSpec()\n",
    "drawing_spec_line.thickness = 1\n",
    "\n",
    "#starting the loop\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.8) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        ret,frame = cap.read()\n",
    "\n",
    "        #make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        #count and show fps\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1/(new_frame_time-prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        fps = int(fps)\n",
    "        fps = str(fps)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(image, fps, (550, 120), font, 3, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "        \n",
    "        #drawing landmarks\n",
    "        draw_landmarks(image, results, drawing_spec_circle, drawing_spec_line)\n",
    "        \n",
    "        #extracting keypoints and feeding them to the LSTM for prediction\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-10:]\n",
    "        \n",
    "        if len(sequence) == 10:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            image = visualize_probabilities(res, actions, image, colors)\n",
    "        \n",
    "        #this would be the rendering logic if one would want to write the action. The first line implies that the action gets\n",
    "        #written only if predicted for 10 consecutive frames, to give an higher stability to the model. This logic is not used\n",
    "        #now, if one would like to implement that, just add 2 lines of code to display a text box with the sentence in\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])    \n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "        #showing image with predictions\n",
    "        cv2.imshow('the feed', image)\n",
    "\n",
    "        #close loop\n",
    "        if cv2.waitKey(10) & 0XFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47127079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
